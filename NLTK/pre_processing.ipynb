{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "- Text cleaning is the process of preparing raw text for NLP (Natural Language Processing) so that machines can understand human language.\n",
    "- Gathering, sorting, and preparing data is the most important step in the data analysis process, bad data can have cumulative negative effects downstream if it is not corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps involved in Text Cleaning / Preprocessing\n",
    "\n",
    "- Tokenization \n",
    "- Normalize Text\n",
    "- Remove Unicode Characters\n",
    "- Remove Stopwords\n",
    "- Perform Stemming and Lemmatization\n",
    "- Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization \n",
    "- Tokenization is the process of splitting a text or a sentence into meaningful units, called tokens. These tokens can be words, phrases, symbols, or other elements, depending on the task. Tokenization is a foundational step in natural language processing (NLP) tasks, enabling computers to process human language effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Tokenization is an important step in natural language processing.\n",
      "Cleaned text: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# python code \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is an important step in natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Text \n",
    "\n",
    "- Normalizing text is the process of standardizing text so that, through NLP, computer models can better understand human input, with the end goal being to more effectively perform any given task.\n",
    "- Specifically, normalizing text with Python and the NLTK library means standardizing capitalization so that machine models donâ€™t group capitalized words (Hey) as different from their lowercase counterparts (hey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, I am trying to Solve Text Normalization Here\n",
      "Cleaned text: hello, i am trying to solve text normalization here\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "# here .lower() is used for standarization. \n",
    "text = \"Hello, I am trying to Solve Text Normalization Here\"\n",
    "text1 = text.lower()\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unicode Characters\n",
    "- Punctuation, Emojiâ€™s, URLâ€™s and @â€™s confuse AI models because they are uniques signatures that either end up being translated unhelpfully into unicode (Smiley face becomes \\u200c or similar), or are unique (in the case of @â€™s and hyperlinks).\n",
    "- Punctuation also creates noise and impedes NLP understanding because it relates to the tone of the specific sentence, not necessarily the word it is attached to.\n",
    "- https?://\\S+: Matches hyperlinks starting with http:// or https://.\n",
    "- [\\U0001F600-\\U0001F64F ... \\U0001F1E0-\\U0001F1FF]: Matches a wide range of emojis and special symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, check this link: https://example.com ðŸ˜Š. Here is another one: http://test.org and emojis like ðŸ˜€, ðŸ˜‚, ðŸ¥³.\n",
      "Cleaned text: Hello, check this link:  . Here is another one:  and emojis like , , .\n"
     ]
    }
   ],
   "source": [
    "# Python code \n",
    "# here Regular expressions or \"re\" can be used for filering\n",
    "\n",
    "import re\n",
    "text = \"Hello, check this link: https://example.com ðŸ˜Š. Here is another one: http://test.org and emojis like ðŸ˜€, ðŸ˜‚, ðŸ¥³.\"\n",
    "pattern = r'(https?://\\S+|[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF\\U0001F1E0-\\U0001F1FF])'\n",
    "cleaned_text = re.sub(pattern, '', text)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing StopWords\n",
    "\n",
    "- Stop words are common words within sentences that do not add value and thus can be eliminated when cleaning for NLP prior to analysis.4\n",
    "- We will have to use NLTK library where we can import pre programmed stop words library. You can create your own stopwords as well as per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ishan-\n",
      "[nltk_data]     pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# python code \n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Here I am trying to see if the stop words are being removed or not\n",
      "Cleaned text: Here I trying see stop words removed\n"
     ]
    }
   ],
   "source": [
    "stop_word = stopwords.words('english')\n",
    "text = \"Here I am trying to see if the stop words are being removed or not\"\n",
    "cleaned_text = \" \".join([word for word in text.split() if word not in stop_word])\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here You can note that the word \"<b> not </b>\" is also being removed which might carry very importnat meaning while solving sentimen analysis task. So while removing stopwords you have to keep this thing in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Stemming and Lemmatization\n",
    "\n",
    "##### Stemming:\n",
    "Stemming is the process of reducing words to their word stem or root form. It involves chopping off the end of words using simple rules, often based on common suffixes. The goal is to reduce related words to a common base form, even if the stem itself may not be a valid word in the language.\n",
    "* \"fishing\" could become \"fish\".\n",
    "* \"cats\" might become \"cat\".\n",
    "\n",
    "Stemming is fast and efficient but may not always result in a valid word. It's useful in tasks where speed and simplicity are prioritized over grammatical correctness.\n",
    "\n",
    "##### Lemmatization: \n",
    "Lemmatization, on the other hand, is the process of reducing words to their base or dictionary form (known as the lemma). Unlike stemming, lemmatization considers the context and meaning of the word. It involves resolving words to their canonical form based on dictionary definitions and grammatical rules of the language.\n",
    "* \"better\" becomes \"good\".\n",
    "\n",
    "Lemmatization requires more computational resources and linguistic knowledge compared to stemming, but it typically results in more accurate and meaningful word forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\n",
      "\n",
      "Stemmed Sentence:\n",
      "he wa run and eat at same time . he ha bad habit of swim after play long hour in the sun .\n",
      "\n",
      "Lemmatized Sentence:\n",
      "He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "print(\"\\nStemmed Sentence:\")\n",
    "print(\" \".join(stemmed_words))\n",
    "print(\"\\nLemmatized Sentence:\")\n",
    "print(\" \".join(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech (POS) Tagging\n",
    "- It is a process of converting a sentence to forms â€” list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('Nepal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('land', 'NN'), ('of', 'IN'), ('Mount', 'NNP'), ('Enerest', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "text = \"Nepal is a land of Mount Enerest\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Parts of Speech: \",nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the above mentioned cleaning and preprocessing steps using a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ishan-\n",
      "[nltk_data]     pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/ishan-pc/Desktop/Ishan-Github/NLP-projects/NLTK/Restaurant_Reviews.tsv\",sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             Wow... Loved this place.\n",
      "1                                   Crust is not good.\n",
      "2            Not tasty and the texture was just nasty.\n",
      "3    Stopped by during the late May bank holiday of...\n",
      "4    The selection on the menu was great and so wer...\n",
      "Name: Review, dtype: object\n",
      "--------------------\n",
      "wow love place\n",
      "crust good\n",
      "tasti textur nasti\n",
      "stop late may bank holiday rick steve recommend love\n",
      "select menu great price\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "print(df[\"Review\"][:5])\n",
    "print(\"-\"*20)\n",
    "\n",
    "for i in range(0,5): \n",
    "    \n",
    "    review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=df['Review'][i])\n",
    "    \n",
    "    review = review.lower()\n",
    "    \n",
    "    review =word_tokenize(review)\n",
    "    \n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review]\n",
    "    \n",
    "    review = ' '.join(review)  \n",
    "    print(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had used stemming for the reviews which has generated the words that doesnt make sense sometimes. Lets use lemmatization and check if there is any difference notieced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             Wow... Loved this place.\n",
      "1                                   Crust is not good.\n",
      "2            Not tasty and the texture was just nasty.\n",
      "3    Stopped by during the late May bank holiday of...\n",
      "4    The selection on the menu was great and so wer...\n",
      "Name: Review, dtype: object\n",
      "--------------------\n",
      "wow loved place\n",
      "crust good\n",
      "tasty texture nasty\n",
      "stopped late may bank holiday rick steve recommendation loved\n",
      "selection menu great price\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "print(df[\"Review\"][:5])\n",
    "print(\"-\"*20)\n",
    "\n",
    "for i in range(0,5): \n",
    "    \n",
    "    review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=df['Review'][i])\n",
    "    \n",
    "    review = review.lower()\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "    \n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    wl = WordNetLemmatizer()\n",
    "    review = [wl.lemmatize(word) for word in review]\n",
    "    \n",
    "    review = ' '.join(review)  \n",
    "    print(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can already see the new generated corpus makes sence and the words are also accuractely represented. \n",
    "- We can also see there is another problem that we have used stopwords and the word not was also used and was removed. \n",
    "- The original text was <b>Crust is not good.</b> But we got.<b> crust good </b> which is completely opposite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our own stopwords library\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = default_stopwords.copy()\n",
    "custom_stopwords.discard('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             Wow... Loved this place.\n",
      "1                                   Crust is not good.\n",
      "2            Not tasty and the texture was just nasty.\n",
      "3    Stopped by during the late May bank holiday of...\n",
      "4    The selection on the menu was great and so wer...\n",
      "Name: Review, dtype: object\n",
      "--------------------\n",
      "wow loved place\n",
      "crust not good\n",
      "not tasty texture nasty\n",
      "stopped late may bank holiday rick steve recommendation loved\n",
      "selection menu great price\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "print(df[\"Review\"][:5])\n",
    "print(\"-\"*20)\n",
    "\n",
    "for i in range(0,5): \n",
    "    \n",
    "    review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=df['Review'][i])\n",
    "    \n",
    "    review = review.lower()\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "    \n",
    "    review = [word for word in review if not word in set(custom_stopwords)]\n",
    "    \n",
    "    wl = WordNetLemmatizer()\n",
    "    review = [wl.lemmatize(word) for word in review]\n",
    "    \n",
    "    review = ' '.join(review)  \n",
    "    print(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"Not\" has not been removed as we have created our won stopword sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
