{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7ebe51d484c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Most', 'of', 'the', 'outlay', 'will', 'be', 'at', 'home', '.', 'No', 'surprise', 'there', ',', 'either', '.', 'While', 'Samsung', 'has', 'expanded', 'overseas', ',', 'South', 'Korea', 'is', 'still', 'host', 'to', 'most', 'of', 'its', 'factories', 'and', 'research', 'engineers', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from collections import Counter\n",
    "text = \"\"\"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. \"\"\"\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc]\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.,\n",
       " It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.\"\"\"\n",
    "\n",
    "word = nlp(text)\n",
    "list(word.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outlay', 'home', 'surprise', 'Samsung', 'expanded', 'overseas', 'South', 'Korea', 'host', 'factories', 'research', 'engineers']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. \"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While while\n",
      "Samsung Samsung\n",
      "has have\n",
      "expanded expand\n",
      "overseas overseas\n",
      ", ,\n",
      "South South\n",
      "Korea Korea\n",
      "is be\n",
      "still still\n",
      "host host\n",
      "to to\n",
      "most most\n",
      "of of\n",
      "its its\n",
      "factories factory\n",
      "and and\n",
      "research research\n",
      "engineers engineer\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. \"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token , token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('outlay', 1), ('home', 1), ('surprise', 1), ('Samsung', 1), ('expanded', 1)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. \"\"\"\n",
    "doc = nlp(text)\n",
    "#remove stopwords and punctuations\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ishan-pc/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a demo text for nlp using nltk. full form of nltk is natural language toolkit\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "lower = text.lower()\n",
    "print(lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'Demo', 'Text', 'for', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a Demo Text for NLP using NLTK.', 'Full form of NLTK is Natural Language Toolkit']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.sent_tokenize(text)\n",
    "print (list(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Demo', 'Text', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword = stopwords.words(\"english\")\n",
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stop = [word for word in word_tokens if word not in stopword]\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopword = stopwords.words(\"english\")\n",
    "lemma = WordNetLemmatizer()\n",
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "tokenss = nltk.word_tokenize(text)\n",
    "lemmaaa = [lemma.lemmatize(word) for word in tokenss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
